{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.0-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (309 kB)\n",
      "Using cached pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, numpy, jiter, idna, h11, distro, certifi, annotated-types, pydantic-core, opencv-python, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 certifi-2024.8.30 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.0 idna-3.10 jiter-0.8.0 numpy-2.1.3 openai-1.55.3 opencv-python-4.10.0.84 pydantic-2.10.2 pydantic-core-2.27.1 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import openai.pydantic_function_tool as pft\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Get the default frame width and height\n",
    "frame_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Write the frame to the output file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the captured frame\n",
    "    cv2.imshow('Camera', frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Encode the current frame as a base64 string\n",
    "    _, buffer = cv2.imencode('.jpg', frame)\n",
    "    image_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Is the fist open or closed?\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "    if \"open\" in response.choices[0].message.content:\n",
    "        print(\"Move forward!\")\n",
    "    elif \"closed\" in response.choices[0].message.content:\n",
    "        print(\"Stop!\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Release the capture and writer objects\n",
    "cam.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify actions as tools\n",
    "\n",
    "class Move(BaseModel):\n",
    "  steps: int\n",
    "\n",
    "class Turn(BaseModel):\n",
    "  rotation: int\n",
    "\n",
    "class Bark(BaseModel):\n",
    "  pass\n",
    "\n",
    "tools = [pft(Move), pft(Turn), pft(Bark)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio():\n",
    "    return 'This is a test audio text'\n",
    "\n",
    "def get_image():\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Write the frame to the output file\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the captured frame\n",
    "    cv2.imshow('Camera', frame)\n",
    "\n",
    "    # Encode the current frame as a base64 string\n",
    "    _, buffer = cv2.imencode('.jpg', frame)\n",
    "    image_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "    return image_base64\n",
    "\n",
    "def pupper_action_step(prev_summary, audio_text=get_audio(), image=get_image(), ):\n",
    "\n",
    "    # audio_text: str\n",
    "    # image: base64 string\n",
    "    # prev_summary: str\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful robot assistant. Use the supplied tools to assist the user.\"\n",
    "    })\n",
    "    messages.append({\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "        },\n",
    "    })\n",
    "    messages.append({\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "        },\n",
    "    })\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-2024-08-06',\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "    response.choices[0].message.tool_calls[0].function\n",
    "    \n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "summary = ''\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    audio_text = get_audio_text()\n",
    "    image = get_image()\n",
    "\n",
    "    summary = pupper_action_step(audio_text, image_base64, summary)\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",x\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Is the fist open or closed?\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "messages = []\n",
    "messages.append({\n",
    "    \"role\": \"system\", \n",
    "    \"content\": \"You are a helpful robot assistant. Use the supplied tools to assist the user.\"\n",
    "})\n",
    "messages.append({\n",
    "    \"type\": \"image_url\",\n",
    "    \"image_url\": {\n",
    "        \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "    },\n",
    "})\n",
    "messages.append({\n",
    "    \"type\": \"image_url\",\n",
    "    \"image_url\": {\n",
    "        \"url\":  f\"data:image/jpeg;base64,{image_base64}\"\n",
    "    },\n",
    "})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o-2024-08-06',\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.choices[0].message.tool_calls[0].function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
